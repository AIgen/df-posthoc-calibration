{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b2fc59-7739-427e-8334-b096a1a4c92a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import seaborn as sns \n",
    "\n",
    "import assessment \n",
    "import calibration \n",
    "import utils \n",
    "from utils import logit, sigmoid\n",
    "import warnings \n",
    "warnings.filterwarnings(\"default\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%matplotlib inline\n",
    "%autoreload 2\n",
    "\n",
    "# Set plotting aesthetics\n",
    "\n",
    "mpl.rcParams.update({'font.size': 15})\n",
    "mpl.rcParams.update({\"axes.grid\" : True, \"grid.linestyle\": '--', \n",
    "                     \"grid.alpha\": 0.8, \"grid.color\": \"black\"})\n",
    "mpl.rcParams.update({\"lines.linewidth\" : 3})\n",
    "mpl.style.use('seaborn-colorblind')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1a6b7f-9c12-445f-825e-bd2d895fd1c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_train = 1000\n",
    "n_val = 500\n",
    "n_test = 4500\n",
    "\n",
    "np.random.seed(0)\n",
    "from scipy.stats import norm\n",
    "\n",
    "def regression_function(x, bias):\n",
    "    p_x_y_0 = (1-bias)*norm.pdf(x)\n",
    "    p_x_y_1 = bias*norm.pdf(x-2)\n",
    "    p_y_equals_1_given_x = p_x_y_1/(p_x_y_0+p_x_y_1)\n",
    "    return p_y_equals_1_given_x\n",
    "\n",
    "def create_data(n):    \n",
    "    dat = np.zeros((n,1))\n",
    "    Y = np.zeros((n))\n",
    "    probs = np.zeros((n))\n",
    "    for i in range(n):\n",
    "        bias = (1-create_data.count/6000)*0.95 + (create_data.count/6000)*0.05\n",
    "        Y[i] = int(np.random.random() <= bias)\n",
    "        dat[i,:] = 2*Y[i] + np.random.randn((1))\n",
    "        probs[i] = regression_function(dat[i,:], bias)\n",
    "                \n",
    "        create_data.count += 1\n",
    "    return dat, Y, probs\n",
    "\n",
    "create_data.count = 0\n",
    "    \n",
    "x_train, y_train, probs_train = create_data(n_train)\n",
    "x_calib, y_calib, probs_calib = create_data(n_val)\n",
    "x_test, y_test, probs_test = create_data(n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71797150-f17b-42bc-867e-22030f1d294c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression().fit(x_train, y_train)\n",
    "_, a_platt, b_platt = calibration.fit_platt_scaling_parameters(logit(model.predict_proba(x_calib)[:,1]), y_calib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160f51ff-8d6f-4f90-b82b-7017e62e124c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probs_ons, a_platt_ons, b_platt_ons = calibration.online_platt_scaling_newton(logit(model.predict_proba(np.vstack((x_calib, x_test)))[:,1]), np.concatenate((y_calib, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196cfc90-378c-4108-99e2-0883e5cbe251",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(a_platt_ons, label=\"a ONS\")\n",
    "plt.plot(b_platt_ons, label=\"b ONS\")\n",
    "plt.legend()\n",
    "plt.title(\"a_fixed = {:.2}, b_fixed = {:.2}\".format(a_platt[0], b_platt[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3570c25d-715a-474f-920b-3db48b0ce07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize=(12, 10))\n",
    "x_vals = np.arange(-3, 5, 0.1).reshape((-1,1))\n",
    "\n",
    "#plt.plot([], [])\n",
    "\n",
    "t1 = 0\n",
    "true_bias = (1-t1/6000)*0.95 + (t1/6000)*0.05\n",
    "true_probs = regression_function(x_vals, true_bias)\n",
    "ax[0,0].plot([], []) \n",
    "ax[0,0].plot([], []) \n",
    "ax[0,0].plot([], [])\n",
    "ax[0,0].plot([], []) \n",
    "ax[0,0].plot(x_vals, true_probs, label = \"True probabilities at t = 0\", linestyle='solid') \n",
    "ax[0,0].plot([], [])\n",
    "ax[0,0].plot(x_vals, model.predict_proba(x_vals)[:,1], label = \"Fixed base model\", linestyle='dotted')#, color=line.get_color())\n",
    "utils.normalized_hist(x_train, ax[0,0])\n",
    "ax[0,0].set_title(r'$t = 1$ to $t = 1000$ (training)')\n",
    "\n",
    "t1 = 1500\n",
    "true_bias = (1-t1/6000)*0.95 + (t1/6000)*0.05\n",
    "true_probs = regression_function(x_vals, true_bias)\n",
    "ax[0,1].plot([], []) \n",
    "ax[0,1].plot([], []) \n",
    "ax[0,1].plot([], [])\n",
    "ax[0,1].plot([], []) \n",
    "ax[0,1].plot(x_vals, true_probs, label = \"True probabilities at t = 1500\", linestyle='solid')\n",
    "ax[0,1].plot([], []) \n",
    "ax[0,1].plot(x_vals, model.predict_proba(x_vals)[:,1], label = \"Fixed base model\", linestyle='dotted')#, color=line.get_color())\n",
    "ax[0,1].plot([], []) \n",
    "ax[0,1].plot(x_vals, sigmoid(a_platt_ons[t1-1000]*logit(model.predict_proba(x_vals)[:,1]) + b_platt_ons[t1-1000]), label = \"Online Platt scaling (OPS) at t = 1500\", linestyle='dashed')\n",
    "utils.normalized_hist(x_test[t1-1500:t1-1000], ax[0,1])\n",
    "ax[0,1].set_title(r'$t = 1501$ to $t = 2000$')\n",
    "\n",
    "t1 = 3500\n",
    "true_bias = (1-t1/6000)*0.95 + (t1/6000)*0.05\n",
    "true_probs = regression_function(x_vals, true_bias)\n",
    "ax[1,0].plot([], []) \n",
    "ax[1,0].plot([], []) \n",
    "ax[1,0].plot([], [])\n",
    "ax[1,0].plot([], []) \n",
    "ax[1,0].plot(x_vals, true_probs, label = \"True probabilities at t = 1500\", linestyle='solid')\n",
    "ax[1,0].plot([], []) \n",
    "ax[1,0].plot(x_vals, model.predict_proba(x_vals)[:,1], label = \"Fixed base model\", linestyle='dotted')#, color=line.get_color())\n",
    "ax[1,0].plot([], []) \n",
    "ax[1,0].plot(x_vals, sigmoid(a_platt_ons[t1-1000]*logit(model.predict_proba(x_vals)[:,1]) + b_platt_ons[t1-1000]), label = \"Online Platt scaling (OPS) at t = 1500\", linestyle='dashed')\n",
    "utils.normalized_hist(x_test[t1-1500:t1-1000], ax[1,0])\n",
    "ax[1,0].set_title(r'$t = 3501$ to $t = 4000$')\n",
    "\n",
    "t1 = 5500\n",
    "true_bias = (1-t1/6000)*0.95 + (t1/6000)*0.05\n",
    "true_probs = regression_function(x_vals, true_bias)\n",
    "ax[1,1].plot([], []) \n",
    "ax[1,1].plot([], []) \n",
    "ax[1,1].plot([], [])\n",
    "ax[1,1].plot([], []) \n",
    "ax[1,1].plot(x_vals, true_probs, label = \"True probabilities at t = 1500\", linestyle='solid')\n",
    "ax[1,1].plot([], []) \n",
    "ax[1,1].plot(x_vals, model.predict_proba(x_vals)[:,1], label = \"Fixed base model\", linestyle='dotted')#, color=line.get_color())\n",
    "ax[1,1].plot([], []) \n",
    "ax[1,1].plot(x_vals, sigmoid(a_platt_ons[t1-1000]*logit(model.predict_proba(x_vals)[:,1]) + b_platt_ons[t1-1000]), label = \"Online Platt scaling (OPS) at t = 1500\", linestyle='dashed')\n",
    "utils.normalized_hist(x_test[t1-1500:t1-1000], ax[1,1])\n",
    "ax[1,1].set_title(r'$t = 5501$ to $t = 6000$')\n",
    "\n",
    "fig.add_subplot(111, frameon=False)\n",
    "plt.grid(False)\n",
    "plt.tick_params(labelcolor='none', which='both', top=False, bottom=False, left=False, right=False)\n",
    "plt.xlabel(r'Value of $x$', fontsize=22)\n",
    "plt.ylabel(r'True/predicted   Pr$(Y=1 \\mid X = x)$', fontsize=20)\n",
    "plt.savefig('results/label_shift_1d.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64781d56-0364-4ce2-8c7b-1eed762751cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_vals(y_pred_probs, true_probs):\n",
    "    y_pred_classes = (y_pred_probs>=0.5).astype('int')\n",
    "    ece = 0\n",
    "    bin_edges = np.sort(np.unique(y_pred_probs))\n",
    "    bin_assignment = utils.bin_points(y_pred_probs, bin_edges)\n",
    "    tot_elem = 0\n",
    "    for i, bin_edges in enumerate(bin_edges):\n",
    "        bin_idx = (bin_assignment == i)\n",
    "        assert(sum(bin_idx) > 0), \"This assert should pass by construction of the code\"\n",
    "        n_elem = sum(bin_idx)\n",
    "        tot_elem += n_elem\n",
    "        pi_pred = y_pred_probs[bin_idx].mean()\n",
    "        pi_true = true_probs[bin_idx].mean()\n",
    "        ece += n_elem*np.abs(pi_pred-pi_true)\n",
    "    assert(tot_elem == y_pred_probs.size)\n",
    "    ece /= y_pred_probs.size\n",
    "    acc = np.sum(np.multiply(true_probs, y_pred_classes) + np.multiply(1-true_probs,1-y_pred_classes))/y_pred_probs.size    \n",
    "\n",
    "    print(\"{:.2f}\\\\% & {:.2}\".format(100*acc, ece))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5b0c25-3b4b-44e0-a9e3-589acc3d69c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_probs = model.predict_proba(x_train)[:,1]\n",
    "true_probs = probs_train\n",
    "print_vals(y_pred_probs, true_probs)\n",
    "for base_val in [0, 2000, 4000]:\n",
    "    y_pred_probs = model.predict_proba(x_test)[base_val:(base_val+500),1]\n",
    "    true_probs = probs_test[base_val:(base_val+500)]\n",
    "    print_vals(y_pred_probs, true_probs)\n",
    "    y_pred_probs = sigmoid(np.multiply(\n",
    "        a_platt_ons[(base_val + 500):(base_val + 1000)], \n",
    "        logit(model.predict_proba(x_test)[base_val:(base_val+500),1])) + b_platt_ons[base_val+500])\n",
    "    print_vals(y_pred_probs, true_probs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
